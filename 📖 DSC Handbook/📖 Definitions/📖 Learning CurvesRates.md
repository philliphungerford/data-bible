---
type: Definition
title: Learning Curves/Rates
tags:
  - definition
---

# Learning Curves/Rates

[image](image%20(1).md)

In neural network training, learning curves areÂ plots that show how a model's performance (like accuracy or loss) changes as the amount of training data increases.Â They help diagnose issues like overfitting and underfitting and determine if adding more data will improve the model's performance. For loss they show [[ðŸ“– Epoch]] on the x axis and [[ðŸ“– Loss]] on the y axis. 



- **Why they're important:**

    - **Overfitting:**Â If the training loss is much lower than the validation loss, and the validation loss doesn't improve with more data, the model might be overfitting, meaning it's learning the training data too well and not generalizing to new data.Â 

    - **Underfitting:**Â If both training and validation losses are high and don't improve much with more data, the model might be underfitting, meaning it's not complex enough to capture the underlying patterns in the data.Â 

    - **Data Sufficiency:**Â Learning curves help determine if adding more training data will significantly improve model performance.Â 

- **Interpreting Learning Curves:**

    - **Steep Curve:**Â A steep curve indicates that the model learns quickly initially, but the rate of improvement slows down as more data is added.Â 

    - **Flat Curve:**Â A flat curve suggests that the model has reached a plateau, and adding more data may not lead to significant improvements.Â 

- **Types of Learning Curves:**

    - **Diminishing Returns:**Â The rate of improvement decreases as more data is added.Â 

    - **Increasing Returns:**Â The rate of improvement increases as more data is added.Â 

    - **S-Curve:**Â A combination of both diminishing and increasing returns.Â 

    - **Complex Curve:**Â Multiple peaks and plateaus, often seen in complex tasks.

